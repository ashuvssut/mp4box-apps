# mp4box-apps

## mp4box-player

- Video player with on-th-fly fragmentation support using mp4box.js. Don't want to use http range requests for fetching video data? You can write you own data fetcher! Checkout [useCustomBufferFetcher.tsx](./demo-player/src/Player/useCustomBufferFetcher.tsx)

```bash
npm i @knide/mp4box-player
```

For Example usage: Checkout [Player.tsx](./demo-player/src/Player/Player.tsx)

## Contribute

Setup

1. Install Git LFS if you are going to upload videos in `lib/server/src/static/`.
2. Run these

    ```bash
    yarn install
    yarn dev # starts demo-player app
    yarn serve # starts the media sever
    ```

## Why do streaming using mp4box player?
At Züs, we previde a decentralized storage solution where you data is kept secured spread across multiple servers (these are called blobbers - our storage provider) and even Züs cannot access your data. So due to that reason, things like processing video data like fragmentation, transcoding, etc is not possible to do at server. So, mp4box is the only way to go for us to do streaming on the web.

### But why use mp4box though? Why wont traditional streaming methods work?

IIn case you want to understand why mp4box was the "only-way" for us to achieve streaming on our *DApp web-app* by leveraging data coming from multiple blobbers (Züs storage providers), the following explanation can help you understand why other streaming methods won't work for us.

Video is played by using the HTML `<video/>` element. `<video/>` needs any of these two to start playing a video:

1. A video URL: **(Google Drive uses this)**
	 -  No fragmentation needed and no streaming protocol or MSE needed
	 -  When you have a video URL, the browser will take care of all the fragmentation and seeking operations
	 - A video URL points to a single server. So we can't use it for our case because we have data coming from multiple blobbers

2. A SourceBuffer **(Used by YouTube & Loom)**
	- SourceBuffer can only be generated by MSE
	- MSE needs fragmented video data to generate SourceBuffer
	- MSE can only append "good" fragmented video data into the source buffer. If you feed MSE a random fragmented video data byte range, the SourceBuffer will close/exit
	- That's the major reason why MSE CANNOT do seeking because it doesn't know how to append a "random" fragmented video byte-range, simply because it cannot determine where the fragmented data starts from in the given byte range. So here two solutions come to rescue:-
		1. **Streaming protocols** like DASH/HLS (The following points are mainly written to understand DASH)
			- When using DASH, we have the manifest file that exactly tells everything about the fragmented data
			- DASH just has a sole purpose to provide information about the fragmented video data, which DASH players like shaka-player can use to play the video. **FACT:** DASH/HLS players use MSE internally (along with a manifest file that provides information about the prepared fragmented video data present in the server)
			- To play a video using streaming protocols, we have to prepare the streaming video data first.
			- Shaka-packager is able to prepare the streaming data like so:-
				- it takes the mp4/webm video as input
				- it outputs 1 audio stream and 1 (or more) video streams along with a .mpd manifest file. **IMPORTANT FACT:** the manifest file can not be generated separately. It has to be generated along with the audio/video streams
			- We can't use streaming protocols because we need an intermediate server to do all the data processing
		2. **mp4box**
			- mp4box works with MSE internally. It can manage the SourceBuffers for you.
			- mp4box doesn't need any "fragmented" mp4 video data. It just needs normal mp4 video data to work
			- mp4box can perform on the fly fragmentation of the video chunks you provide it
			- when we do seeking in the video, mp4box will exactly fetch the byte range needed and do the fragmentation. Since it itself generates the fragmented video data, we don't have to worry about a manifest file to tell the starting point of the generated fragmented video data
			- The major drawback is that it sometime needs large chunks of video to start fragmentation. Those chunks could worth 30 seconds of the video or even larger. mp4box keeps on requesting more video chunks until it could start doing the fragmentation

---------------
**TL;DR**
- We cannot use the mentioned "video URL" solution because a URL points to a single Server only (not multiple blobbers)
- We cannot use "streaming protocols" solution because we need prepared video data beforehand (In case of DASH: 1 audio stream + 1 or more video streams + .mpd manifest file, in case of HLS: "A LOT" of segmented files + m3u8 manifest file)
- We cannot use MSE because we cannot do video seeking (the reason is mentioned above - we need a manifest file to use the fragmented video data correctly)

mp4box does what the above three cannot do. It uses data from multiple blobbers + it does fragmentation on the fly + it doesn't need a manifest file

## Note

- This project is still in development. The functionality works as shown in the demo player but its stil not production ready because sometimes if the `chunkSize` config is set to a small value (day `<3MB`), then the video fragmentation takes a lot of time.
